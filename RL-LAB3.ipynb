{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Universidad del Valle de Guatemala  \n",
        "Aprendizaje por Refuerzo\n",
        "Alberto Suriano  \n",
        "\n",
        "Laboratorio 3  \n",
        "Marlon Hernández - 15177  \n",
        "\n",
        "- Link del repositorio: https://github.com/ivanhez/RL-LAB3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
        "1. **¿Qué es Programación Dinámica y cómo se relaciona con RL?**  \n",
        "    La programación dinámica es un enfoque para resolver problemas complejos dividiéndolos en subproblemas más simples. Este acercamiento nos ayuda con la iteración de póliza y la iteración de valor, para encontrar el valores o políticas óptimas en entornos donde se conocen las probabilidades de transición y las recompensas.\n",
        "\n",
        "2. **Explique en sus propias palabras el algoritmo de Iteración de Póliza.**  \n",
        "    La iteración de póliza permite encontrar la política óptima, el cual consite en empezar con una política inicial y calcular el valor de dicha política. Los valores de los estados se van actualizando basándose en la política actual hasta que los valores convergan, esto hace que las acciones maximicen el valor esperado en cada estado. Estos pasos se repiten hasta que la política deja de cambiar, lo que indica que se ha encontrado la política óptima.\n",
        "    \n",
        "3. **Explique en sus propias palabras el algoritmo de Iteración de Valor.**  \n",
        "    La iteración de valor empieza con una estimación inicial de los valores de los estados y se actualizan iterativamente. En cada iteración, se calcula el valor de cada estado como el valor máximo esperado de las acciones posibles desde ese estado. Esto continúa hasta que los valores de los estados convergen. esto nos da los valores de la acción que maximiza el valor esperado, obteniendo en sí, la política óptima\n",
        "\n",
        "4. **En el laboratorio pasado, vimos que el valor de los premios obtenidos se mantienen constantes, ¿por qué?**\n",
        "    En el laboratorio anterior, el valor de los premios obtenidos se mantienen constantes debido a que las recompensas son las mismas al definirlas previamente. Ya que no se actualiza el ambiente o las recompensas, siempre se mantiene los valores de los premios definidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2\n",
        "El objetivo principal de este ejercicio es que simule un MDP que represente un robot que navega por un laberinto de cuadrículas de 3x3 y evalúe una política determinada. Por ello considere, a un robot navega por un laberinto de cuadrícula de 3x3. El robot puede moverse en cuatro direcciones: arriba, abajo, izquierda y derecha. El objetivo es navegar desde la posición inicial hasta la posición de meta evitando obstáculos. El robot recibe una recompensa cuando alcanza la meta y una penalización si choca con un obstáculo."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
