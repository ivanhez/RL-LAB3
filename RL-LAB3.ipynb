{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Universidad del Valle de Guatemala  \n",
        "Aprendizaje por Refuerzo\n",
        "Alberto Suriano  \n",
        "\n",
        "Laboratorio 3  \n",
        "Marlon Hernández - 15177  \n",
        "\n",
        "- Link del repositorio: https://github.com/ivanhez/RL-LAB3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
        "1. **¿Qué es Programacion Dinámica y como se relaciona con RL?**  \n",
        "    La programacion dinámica es un enfoque para resolver problemas complejos dividiéndolos en subproblemas más simples. Este acercamiento nos ayuda con la iteracion de poliza y la iteracion de valor, para encontrar el valores o politicas optimas en entornos donde se conocen las probabilidades de transicion y las recompensas.\n",
        "\n",
        "2. **Explique en sus propias palabras el algoritmo de Iteracion de Poliza.**  \n",
        "    La iteracion de poliza permite encontrar la politica optima, el cual consite en empezar con una politica inicial y calcular el valor de dicha politica. Los valores de los estados se van actualizando basándose en la politica actual hasta que los valores convergan, esto hace que las acciones maximicen el valor esperado en cada estado. Estos pasos se repiten hasta que la politica deja de cambiar, lo que indica que se ha encontrado la politica optima.\n",
        "    \n",
        "3. **Explique en sus propias palabras el algoritmo de Iteracion de Valor.**  \n",
        "    La iteracion de valor empieza con una estimacion inicial de los valores de los estados y se actualizan iterativamente. En cada iteracion, se calcula el valor de cada estado como el valor máximo esperado de las acciones posibles desde ese estado. Esto continúa hasta que los valores de los estados convergen. esto nos da los valores de la accion que maximiza el valor esperado, obteniendo en si, la politica optima\n",
        "\n",
        "4. **En el laboratorio pasado, vimos que el valor de los premios obtenidos se mantienen constantes, ¿por qué?**\n",
        "    En el laboratorio anterior, el valor de los premios obtenidos se mantienen constantes debido a que las recompensas son las mismas al definirlas previamente. Ya que no se actualiza el ambiente o las recompensas, siempre se mantiene los valores de los premios definidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2\n",
        "El objetivo principal de este ejercicio es que simule un MDP que represente un robot que navega por un laberinto de cuadriculas de 3x3 y evalúe una politica determinada. Por ello considere, a un robot navega por un laberinto de cuadricula de 3x3. El robot puede moverse en cuatro direcciones: arriba, abajo, izquierda y derecha. El objetivo es navegar desde la posicion inicial hasta la posicion de meta evitando obstáculos. El robot recibe una recompensa cuando alcanza la meta y una penalizacion si choca con un obstáculo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estados S: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
            "Acciones: ['arriba', 'abajo', 'izquierda', 'derecha']\n",
            "Start: 0\n",
            "Goal: 2\n",
            "Obstacles: {8, 4}\n",
            "+---+---+---+\n",
            "| 0 | 1 | 2 |\n",
            "+---+---+---+\n",
            "| 3 | 4 | 5 |\n",
            "+---+---+---+\n",
            "| 6 | 7 | 8 |\n",
            "+---+---+---+\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Estados\n",
        "S = list(range(9))\n",
        "print(f\"Estados S: {S}\")\n",
        "\n",
        "# Acciones\n",
        "A = [\"arriba\", \"abajo\", \"izquierda\", \"derecha\"]\n",
        "print(f\"Acciones: {A}\")\n",
        "\n",
        "start = 0\n",
        "goal = 2\n",
        "obstacles = {4, 8}\n",
        "\n",
        "laberinto = [\n",
        "    ['0', '1', '2'],\n",
        "    ['3', '4', '5'],\n",
        "    ['6', '7', '8']\n",
        "]\n",
        "print(f\"Start: {start}\")\n",
        "print(f\"Goal: {goal}\")\n",
        "print(f\"Obstacles: {obstacles}\")\n",
        "print(\"+---+---+---+\")\n",
        "for fila in laberinto:\n",
        "    print(\"|\", end=\"\")\n",
        "    for celda in fila:\n",
        "        print(f\" {celda} |\", end=\"\")\n",
        "    print(\"\\n+---+---+---+\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de transicion\n",
            "0: {'arriba': {0: 1}, 'abajo': {3: 1}, 'izquierda': {0: 1}, 'derecha': {1: 1}}\n",
            "1: {'arriba': {1: 1}, 'abajo': {1: 1}, 'izquierda': {0: 1}, 'derecha': {2: 1}}\n",
            "2: {'arriba': {2: 1}, 'abajo': {5: 1}, 'izquierda': {1: 1}, 'derecha': {2: 1}}\n",
            "3: {'arriba': {0: 1}, 'abajo': {6: 1}, 'izquierda': {3: 1}, 'derecha': {3: 1}}\n",
            "4: {'arriba': {}, 'abajo': {}, 'izquierda': {}, 'derecha': {}}\n",
            "5: {'arriba': {2: 1}, 'abajo': {5: 1}, 'izquierda': {5: 1}, 'derecha': {5: 1}}\n",
            "6: {'arriba': {3: 1}, 'abajo': {6: 1}, 'izquierda': {6: 1}, 'derecha': {7: 1}}\n",
            "7: {'arriba': {7: 1}, 'abajo': {7: 1}, 'izquierda': {6: 1}, 'derecha': {7: 1}}\n",
            "8: {'arriba': {}, 'abajo': {}, 'izquierda': {}, 'derecha': {}}\n",
            "Funcion de recompensa\n",
            "0: {'arriba': {0: -0.01}, 'abajo': {3: -0.01}, 'izquierda': {0: -0.01}, 'derecha': {1: -0.01}}\n",
            "1: {'arriba': {1: -0.01}, 'abajo': {1: -1}, 'izquierda': {0: -0.01}, 'derecha': {2: 1}}\n",
            "2: {'arriba': {2: 1}, 'abajo': {5: -0.01}, 'izquierda': {1: -0.01}, 'derecha': {2: 1}}\n",
            "3: {'arriba': {0: -0.01}, 'abajo': {6: -0.01}, 'izquierda': {3: -0.01}, 'derecha': {3: -1}}\n",
            "4: {'arriba': {}, 'abajo': {}, 'izquierda': {}, 'derecha': {}}\n",
            "5: {'arriba': {2: 1}, 'abajo': {5: -1}, 'izquierda': {5: -1}, 'derecha': {5: -0.01}}\n",
            "6: {'arriba': {3: -0.01}, 'abajo': {6: -0.01}, 'izquierda': {6: -0.01}, 'derecha': {7: -0.01}}\n",
            "7: {'arriba': {7: -1}, 'abajo': {7: -0.01}, 'izquierda': {6: -0.01}, 'derecha': {7: -1}}\n",
            "8: {'arriba': {}, 'abajo': {}, 'izquierda': {}, 'derecha': {}}\n"
          ]
        }
      ],
      "source": [
        "# Matriz de transicion\n",
        "P = {s: {a: {} for a in A} for s in S}\n",
        "\n",
        "# Funcion de recompensa\n",
        "R = {s: {a: {} for a in A} for s in S}\n",
        "\n",
        "# Llenar las transiciones y recompensas\n",
        "for s in S:\n",
        "    if s in obstacles:\n",
        "        continue\n",
        "    row, col = divmod(s, 3)\n",
        "    neighbors = {\n",
        "        'arriba': s - 3 if row > 0 else s,\n",
        "        'abajo': s + 3 if row < 2 else s,\n",
        "        'izquierda': s - 1 if col > 0 else s,\n",
        "        'derecha': s + 1 if col < 2 else s,\n",
        "    }\n",
        "    for a in A:\n",
        "        next_state = neighbors[a]\n",
        "        if next_state in obstacles:\n",
        "            P[s][a][s] = 1\n",
        "            R[s][a][s] = -1\n",
        "        else:\n",
        "            P[s][a][next_state] = 1\n",
        "            if next_state == goal:\n",
        "                R[s][a][next_state] = 1\n",
        "            else:\n",
        "                R[s][a][next_state] = -0.01\n",
        "\n",
        "\n",
        "print(\"Matriz de transicion\")\n",
        "for key, value in P.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "print(\"Funcion de recompensa\")\n",
        "for key, value in R.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "\n",
        "# Inicializacion de la funcion de valor\n",
        "V = {s: 0 for s in S}\n",
        "\n",
        "# Factor de descuento\n",
        "gamma = 0.9\n",
        "\n",
        "# Minimo de cambio para detener la iteracion\n",
        "theta = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iteracion de valor\n",
        "def value_iteration(V, P, R, S, A, gamma, theta):\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in S:\n",
        "            if s in obstacles:\n",
        "                continue\n",
        "            v = V[s]\n",
        "            V[s] = max(sum(P[s][a][s_prime] * (R[s][a][s_prime] + gamma * V[s_prime])\n",
        "                           for s_prime in P[s][a]) for a in A)\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "# Extraer la politica\n",
        "def extract_policy(V, P, R, S, A, gamma):\n",
        "    policy = {}\n",
        "    for s in S:\n",
        "        if s in obstacles:\n",
        "            policy[s] = None\n",
        "            continue\n",
        "        best_action = None\n",
        "        best_value = float('-inf')\n",
        "        for a in A:\n",
        "            action_value = sum(P[s][a][s_prime] * (R[s][a][s_prime] + gamma * V[s_prime])\n",
        "                               for s_prime in P[s][a])\n",
        "            if action_value > best_value:\n",
        "                best_value = action_value\n",
        "                best_action = a\n",
        "        policy[s] = best_action\n",
        "    return policy\n",
        "\n",
        "\n",
        "# Evaliar la politica\n",
        "def policy_evaluation(policy, V, P, R, S, gamma, theta):\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in S:\n",
        "            if s in obstacles or policy[s] is None:\n",
        "                continue\n",
        "            v = V[s]\n",
        "            a = policy[s]\n",
        "            V[s] = sum(P[s][a][s_prime] * (R[s][a][s_prime] + gamma * V[s_prime]) for s_prime in P[s][a])\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "# Encontrar politica optima\n",
        "def policy_improvement(V, P, R, S, A, gamma):\n",
        "    policy = {}\n",
        "    for s in S:\n",
        "        if s in obstacles:\n",
        "            policy[s] = None\n",
        "            continue\n",
        "        best_action = None\n",
        "        best_value = float('-inf')\n",
        "        for a in A:\n",
        "            action_value = sum(P[s][a][s_prime] * (R[s][a][s_prime] + gamma * V[s_prime])\n",
        "                               for s_prime in P[s][a])\n",
        "            if action_value > best_value:\n",
        "                best_value = action_value\n",
        "                best_action = a\n",
        "        policy[s] = best_action\n",
        "    return policy\n",
        "\n",
        "# Iterar la politica\n",
        "def policy_iteration(P, R, S, A, gamma, theta):\n",
        "    policy = {s: A[0] for s in S if s not in obstacles}\n",
        "    while True:\n",
        "        V = {s: 0 for s in S}\n",
        "        V = policy_evaluation(policy, V, P, R, S, gamma, theta)\n",
        "        new_policy = policy_improvement(V, P, R, S, A, gamma)\n",
        "        if new_policy == policy:\n",
        "            break\n",
        "        policy = new_policy\n",
        "    return V, policy\n",
        "\n",
        "# Encontrar el valor optimo \n",
        "V_optimal = value_iteration(V, P, R, S, A, gamma, theta)\n",
        "\n",
        "# Extraer la politica optima\n",
        "optimal_policy = extract_policy(V_optimal, P, R, S, A, gamma)\n",
        "\n",
        "# Realizar la iteracion de politicas\n",
        "V_policy, optimal_policy_from_policy_iteration = policy_iteration(P, R, S, A, gamma, theta)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Funcion de Valor optima por Iteracion de Valor:\n",
            "Estado 0: Valor = 8.9814\n",
            "Estado 1: Valor = 9.9914\n",
            "Estado 2: Valor = 9.9914\n",
            "Estado 3: Valor = 8.0733\n",
            "Estado 4: Valor = 0.0000\n",
            "Estado 5: Valor = 9.9923\n",
            "Estado 6: Valor = 7.2559\n",
            "Estado 7: Valor = 6.5203\n",
            "Estado 8: Valor = 0.0000\n",
            "\n",
            "Politica optima por Iteracion de Valor:\n",
            "Estado 0: Accion = derecha\n",
            "Estado 1: Accion = derecha\n",
            "Estado 2: Accion = arriba\n",
            "Estado 3: Accion = arriba\n",
            "Estado 4: Accion = Ninguna\n",
            "Estado 5: Accion = arriba\n",
            "Estado 6: Accion = arriba\n",
            "Estado 7: Accion = izquierda\n",
            "Estado 8: Accion = Ninguna\n",
            "\n",
            "Funcion de Valor optima por Iteracion de Politicas:\n",
            "Estado 0: Valor = 8.9814\n",
            "Estado 1: Valor = 9.9914\n",
            "Estado 2: Valor = 9.9914\n",
            "Estado 3: Valor = 8.0733\n",
            "Estado 4: Valor = 0.0000\n",
            "Estado 5: Valor = 9.9923\n",
            "Estado 6: Valor = 7.2559\n",
            "Estado 7: Valor = 6.5203\n",
            "Estado 8: Valor = 0.0000\n",
            "\n",
            "Politica optima por Iteracion de Politicas:\n",
            "Estado 0: Accion = derecha\n",
            "Estado 1: Accion = derecha\n",
            "Estado 2: Accion = arriba\n",
            "Estado 3: Accion = arriba\n",
            "Estado 4: Accion = Ninguna\n",
            "Estado 5: Accion = arriba\n",
            "Estado 6: Accion = arriba\n",
            "Estado 7: Accion = izquierda\n",
            "Estado 8: Accion = Ninguna\n"
          ]
        }
      ],
      "source": [
        "print(\"Funcion de Valor optima por Iteracion de Valor:\")\n",
        "for s in S:\n",
        "    print(f\"Estado {s}: Valor = {V_optimal[s]:.4f}\")\n",
        "\n",
        "print(\"\\nPolitica optima por Iteracion de Valor:\")\n",
        "for s in S:\n",
        "    action = optimal_policy[s]\n",
        "    if action is not None:\n",
        "        print(f\"Estado {s}: Accion = {action}\")\n",
        "    else:\n",
        "        print(f\"Estado {s}: Accion = Ninguna\")\n",
        "\n",
        "\n",
        "print(\"\\nFuncion de Valor optima por Iteracion de Politicas:\")\n",
        "for s in S:\n",
        "    print(f\"Estado {s}: Valor = {V_policy[s]:.4f}\")\n",
        "\n",
        "print(\"\\nPolitica optima por Iteracion de Politicas:\")\n",
        "for s in S:\n",
        "    action = optimal_policy_from_policy_iteration[s]\n",
        "    if action is not None:\n",
        "        print(f\"Estado {s}: Accion = {action}\")\n",
        "    else:\n",
        "        print(f\"Estado {s}: Accion = Ninguna\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
